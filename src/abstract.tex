This study investigates the application of Large Language Models (LLMs) as a support tool for the automated verification of compliance between software requirements and their implementation in source code. It is based on the recognition that traceability between specifications and software artifacts is a critical activity, yet prone to failures, omissions, and rework when performed manually. To address this, the study explores foundational concepts such as semantic vectorization, context windows, prompt engineering, and machine learning principles applied to the domain of software engineering.

The proposed approach was validated through the analysis of a real system, focusing on two modules: one primarily involving backend operations and another integrating backend and frontend components. Audit prompts were designed to guide the models in the automated inspection of repository files, evaluating the presence or absence of implementations consistent with the specified requirements. The outputs generated by the LLMs were compared to human evaluations and analyzed using metrics such as simple accuracy, strict accuracy, omission rate, precision, recall, and F1-score.

The results demonstrated significant performance by the models, with strict accuracy exceeding 90\% in certain scenarios, highlighting the feasibility of the approach as a support mechanism for compliance analysis. Despite these advances, the solution still presents limitations, such as context window constraints and the need for careful curation of the analyzed files. Future work includes integration with autonomous agents based on the MCP protocol to automate corrective actions, as well as the adoption of Retrieval-Augmented Generation (RAG) techniques to overcome limitations imposed by large code repositories.

